{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "import os.path\n",
    "\n",
    "# Downloading stopwords (just a mock step since we don't have internet access here)\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, lowercase, and remove stopwords from the text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "def multiprocess_preprocess_joblib(data, column_name):\n",
    "    \"\"\"Preprocess data using multiple processes with joblib.\"\"\"\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Using joblib's Parallel and delayed to run preprocessing in parallel\n",
    "    processed_data = Parallel(n_jobs=num_processes)(delayed(preprocess_text)(text) for text in data[column_name])\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# We'll use a predefined list of English stopwords.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# if data/preprocessed_collection.tsv does not exist, create it\n",
    "if not os.path.isfile('data/preprocessed_collection.tsv'):\n",
    "    dataset = pd.read_csv('data/collection.tsv', sep='\\t')\n",
    "    dataset['processed_text'] = multiprocess_preprocess_joblib(dataset, 'text')\n",
    "    dataset = dataset.drop(columns=['text'])\n",
    "    dataset = dataset.rename(columns={'processed_text': 'text'})\n",
    "    dataset.to_csv('data/preprocessed_collection.tsv', sep='\\t', index=False)\n",
    "else:\n",
    "    dataset = pd.read_csv('data/preprocessed_collection.tsv', sep='\\t')\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "if not os.path.isfile('data/preprocessed_querys_train.tsv'):\n",
    "    train_querys = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "    train_querys['processed_query'] = multiprocess_preprocess_joblib(train_querys, 'query')\n",
    "    train_querys = train_querys.drop(columns=['query'])\n",
    "    train_querys = train_querys.rename(columns={'processed_query': 'query'})\n",
    "    train_querys.to_csv('data/preprocessed_queries_train.csv', index=False)\n",
    "else: \n",
    "    train_querys = pd.read_csv('data/preprocessed_queries_train.csv')\n",
    "\n",
    "\n",
    "if not os.path.isfile('data/preprocessed_querys_test.tsv'):\n",
    "    test_querys = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "    test_querys['processed_query'] = multiprocess_preprocess_joblib(test_querys, 'query')\n",
    "    test_querys = test_querys.drop(columns=['query'])\n",
    "    test_querys = test_querys.rename(columns={'processed_query': 'query'})\n",
    "    test_querys.to_csv('data/preprocessed_queries_test.csv', index=False)\n",
    "else:\n",
    "    test_querys = pd.read_csv('data/preprocessed_queries_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "\n",
    "tokenized_passages = dataset['text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# if piccke index is not present load it\n",
    "if not os.path.isfile('data/bm25_index.pickle'):\n",
    "    print(\"Index data not present, creating...\")\n",
    "    bm25 = BM25Okapi(tokenized_passages)\n",
    "    with open('data/bm25_index.pickle', 'wb') as handle:\n",
    "        pickle.dump(bm25, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('data/bm25_index.pkl', 'rb') as f:\n",
    "        bm25 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/248 [00:27<1:51:19, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved all passages for query 1_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/248 [01:22<5:39:08, 82.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/Uni/Text mining/Project/conversational_passage_retrieval/baseline.ipynb Cella 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m ranking_results_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m tqdm(test_querys\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39mtest_querys\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     qid, top_indices \u001b[39m=\u001b[39m retrieve_rankings(row)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     ranking_results_dict[qid] \u001b[39m=\u001b[39m top_indices\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Generate the TREC runfile using the results\u001b[39;00m\n",
      "\u001b[1;32m/Users/user/Desktop/Uni/Text mining/Project/conversational_passage_retrieval/baseline.ipynb Cella 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m qid \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mqid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m query_text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m top_indices \u001b[39m=\u001b[39m bm25\u001b[39m.\u001b[39;49mget_top_n(query_text\u001b[39m.\u001b[39;49msplit(), \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(dataset)), n\u001b[39m=\u001b[39;49mtop_k)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRetrieved all passages for query \u001b[39m\u001b[39m{\u001b[39;00mqid\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/Uni/Text%20mining/Project/conversational_passage_retrieval/baseline.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m qid, top_indices\n",
      "File \u001b[0;32m~/test/pytorch-ane/env/lib/python3.9/site-packages/rank_bm25.py:73\u001b[0m, in \u001b[0;36mBM25.get_top_n\u001b[0;34m(self, query, documents, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_top_n\u001b[39m(\u001b[39mself\u001b[39m, query, documents, n\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m     71\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_size \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(documents), \u001b[39m\"\u001b[39m\u001b[39mThe documents given don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the index corpus!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_scores(query)\n\u001b[1;32m     74\u001b[0m     top_n \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(scores)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:n]\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m [documents[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m top_n]\n",
      "File \u001b[0;32m~/test/pytorch-ane/env/lib/python3.9/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36mBM25Okapi.get_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([(doc\u001b[39m.\u001b[39mget(q) \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_freqs])\n\u001b[1;32m    119\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf\u001b[39m.\u001b[39mget(q) \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m (q_freq \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk1 \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk1 \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m*\u001b[39m doc_len \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/test/pytorch-ane/env/lib/python3.9/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([(doc\u001b[39m.\u001b[39;49mget(q) \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_freqs])\n\u001b[1;32m    119\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf\u001b[39m.\u001b[39mget(q) \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m (q_freq \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk1 \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk1 \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m*\u001b[39m doc_len \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_trec_runfile(ranking_results, run_identifier, output_file):\n",
    "    \"\"\"Generate a TREC runfile using the given ranking results.\"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        for qid, passage_indices in ranking_results.items():\n",
    "            for rank, passage_idx in enumerate(passage_indices, 1):\n",
    "                # Construct the turn identifier from the qid\n",
    "                topic_id, turn_id = qid.split(\"_\")\n",
    "                turn_identifier = f\"{topic_id}_{turn_id}\"\n",
    "                \n",
    "                # Retrieve the document ID from the collection using the passage index\n",
    "                doc_id = dataset.iloc[passage_idx]['pid']\n",
    "                \n",
    "                # The BM25 score could be retrieved and used here, but for simplicity, \n",
    "                # we're using the rank as a negative score (to ensure descending order)\n",
    "                score = -rank\n",
    "                \n",
    "                # Write the formatted line to the file\n",
    "                file.write(f\"{turn_identifier} Q0 {doc_id} {rank} {score} {run_identifier}\\n\")\n",
    "\n",
    "\n",
    "# Retrieve top 1000 passages for each query in test_querys\n",
    "ranking_results_new = {}\n",
    "top_k = 1000\n",
    "run_id_new = \"BM25_integration_run\"\n",
    "\n",
    "\n",
    "def retrieve_rankings(row):\n",
    "    qid = row['qid']\n",
    "    query_text = row['query']\n",
    "    top_indices = bm25.get_top_n(query_text.split(), range(len(tokenized_passages)), n=top_k)\n",
    "    print(f\"Retrieved all passages for query {qid}\")\n",
    "    return qid, top_indices\n",
    "from tqdm import tqdm\n",
    "\n",
    "ranking_results_dict = {}\n",
    "for _, row in tqdm(test_querys.iterrows(), total=test_querys.shape[0]):\n",
    "    qid, top_indices = retrieve_rankings(row)\n",
    "    ranking_results_dict[qid] = top_indices\n",
    "\n",
    "# Generate the TREC runfile using the results\n",
    "output_filename_parallel = \"/data/trec_runfile_parallel.txt\"\n",
    "generate_trec_runfile(ranking_results_dict, run_id_new, output_filename_parallel)\n",
    "\n",
    "output_filename_parallel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
